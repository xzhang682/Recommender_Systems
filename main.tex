\documentclass{article}

\usepackage{amssymb}
\usepackage{amsmath, amsthm, color}
\usepackage[colorlinks]{hyperref}
\usepackage{fullpage}
\usepackage{verbatim}

\usepackage{lineno}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=magenta,      
    urlcolor=black,
    pdftitle={Sharelatex Example},
    bookmarks=true,
    pdfpagemode=FullScreen,
    }
%\linenumbers



\newtheorem{theorem}{Theorem}
\newtheorem{question}[theorem]{Question}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}[theorem]{Proposition}

% % the following creates defns with the usual roman font, not in italics
\newtheorem{definition}{Definition}
\newtheorem{fact}{Fact}[section]
\newtheorem{assumption}{Assumption}

% Not in italics
\theoremstyle{definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}

% WIDEBAR COMMAND
\newlength{\widebarargwidth}
\newlength{\widebarargheight}
\newlength{\widebarargdepth}
\DeclareRobustCommand{\widebar}[1]{%
  \settowidth{\widebarargwidth}{\ensuremath{#1}}%
  \settoheight{\widebarargheight}{\ensuremath{#1}}%
  \settodepth{\widebarargdepth}{\ensuremath{#1}}%
  \addtolength{\widebarargwidth}{-0.3\widebarargheight}%
  \addtolength{\widebarargwidth}{-0.3\widebarargdepth}%
  \makebox[0pt][l]{\hspace{0.3\widebarargheight}%
    \hspace{0.3\widebarargdepth}%
    \addtolength{\widebarargheight}{0.3ex}%
    \rule[\widebarargheight]{0.95\widebarargwidth}{0.1ex}}%
  {#1}}

% Notational convenience

\newcommand{\E}{\ensuremath{\mathbb{E}}}
\newcommand{\mprob}{\ensuremath{\mathbb{P}}}
\newcommand{\scriptW}{\ensuremath{\mathcal{W}}}
\newcommand{\real}{\ensuremath{\mathbb{R}}}
\newcommand{\muhat}{\ensuremath{\widehat{\mu}}}
\newcommand{\barw}{\ensuremath{\bar{w}}}
\newcommand{\scriptE}{\ensuremath{\mathcal{E}}}
\newcommand{\scriptT}{\ensuremath{\mathcal{T}}}
\newcommand{\sigmatil}{\ensuremath{\widetilde{\sigma}}}
\newcommand{\order}{\ensuremath{\mathcal{O}}}
\newcommand{\scriptA}{\ensuremath{\mathcal{A}}}
\newcommand{\mustar}{\ensuremath{\mu^*}}
\newcommand{\thetabar}{\ensuremath{\widebar{\theta}}}
\newcommand{\poly}{\ensuremath{\operatorname{poly}}}
\newcommand{\Var}{\ensuremath{\operatorname{Var}}}
\newcommand{\Otil}{\ensuremath{\widetilde{O}}}
\newcommand{\sigmahat}{\ensuremath{\widehat{\sigma}}}
\newcommand{\betastar}{\ensuremath{\beta^*}}
\newcommand{\gammastar}{\ensuremath{\gamma^*}}
\newcommand{\betahat}{\ensuremath{\widehat{\beta}}}
\newcommand{\gammahat}{\ensuremath{\widehat{\gamma}}}
\newcommand{\nuhat}{\ensuremath{\widehat{\nu}}}
\newcommand{\xihat}{\ensuremath{\widehat{\xi}}}
\newcommand{\supp}{\ensuremath{\operatorname{supp}}}
\newcommand{\Ttil}{\ensuremath{\widetilde{T}}}
\newcommand{\inprod}[2]{\ensuremath{\langle #1 , \, #2 \rangle}}
\newcommand{\Ball}{\ensuremath{\mathbb{B}}}
\newcommand{\Xtil}{\ensuremath{\widetilde{X}}}
\newcommand{\Atil}{\ensuremath{\widetilde{A}}}
\newcommand{\opnorm}[1]{\left|\!\left|\!\left|{#1}\right|\!\right|\!\right|}




\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{xcolor}
\usepackage{graphicx}
\title{Recommender Systems Summary}


\author{Xiaomin Zhang $\quad $ \href{mailto:xzhang682@wisc.edu}{xzhang682@wisc.edu} }
% \author{
%   David S.~Hippocampus\thanks{Use footnote for providing further
%     information about author (webpage, alternative
%     address)---\emph{not} for acknowledging funding agencies.} \\
%   Department of Computer Science\\
%   Cranberry-Lemon University\\
%   Pittsburgh, PA 15213 \\
%   \texttt{hippo@cs.cranberry-lemon.edu} \\
%   %% examples of more authors
%   \And
%  Elias D.~Striatum \\
%   Department of Electrical Engineering\\
%   Mount-Sheikh University\\
%   Santa Narimana, Levand \\
%   \texttt{stariate@ee.mount-sheikh.edu} \\
%   %% \AND
%   %% Coauthor \\
%   %% Affiliation \\
%   %% Address \\
%   %% \texttt{email} \\
%   %% \And
%   %% Coauthor \\
%   %% Affiliation \\
%   %% Address \\
%   %% \texttt{email} \\
%   %% \And
%   %% Coauthor \\
%   %% Affiliation \\
%   %% Address \\
%   %% \texttt{email} \\
% }

\begin{document}
\maketitle

% \begin{abstract}
% \lipsum[1]
% \end{abstract}


% % keywords can be removed
% \keywords{First keyword \and Second keyword \and More}

\section{User-User Collaborative Filtering}

\subsection{Main Contents}
\textbf{References:} An Algorithmic Framwork for Collaborative Filtering by Herlocker, Konstan, Borchers, and Riedl (Proc. SIGIR 1999).

Let $U$ be a set of users and $I$ be a set of items. Suppose we have a collection of ratings $r_{u,i}$ for some user $u$ rating some item $i$. Note that the rating matrix $R$ is assumed to be sparse. A simple prediction score is defined as follow,
\begin{align}
    s(u,i) = \frac{\sum_{v \in U} r_{v, i} }{|U|}.
\end{align}

To weight the ratings differently, a variant is as follow
\begin{align}
    s(u,i) = \frac{\sum_{v \in U} r_{v, i} \cdot w_{u,v} }{\sum_{v\in U} w_{u, v}},
\end{align}
where $w_{u,v}$ is a similarity/weighting parameter between user $u$ and $v$. 

For the problem that users have different rating scales, letting $\bar{r}_u$ be the average score for some user $u$, we can introduce the following variant,
\begin{align}
    s(u,i) = \bar{r}_{u} + \frac{\sum_{v\in U} (r_{v, i} - \bar{r}_v)}{|U|}.
\end{align}

Combining the two variants we get 
\begin{align}
    s(u,i) = \bar{r}_{u} + \frac{\sum_{v\in U} (r_{v, i} - \bar{r}_v) \cdot w_{u, v}}{\sum_{v\in U} w_{u, v}}.
\end{align}
In particular, the weighting parameter can be defined as the Pearson correlation which is defined as 
\begin{align*}
    w_{u,v} = \frac{\sum_{i \in I} (r_{u,i} - r_u)(r_{v,i} - r_v)}{\sigma_u \cdot \sigma_v}.
\end{align*}

UUCF is a personalized recommendations/predictions.

The base assumptions of the UUCF are: Our tastes are either individually stable or move in sync with each other; Our system is scoped within a domain of agreement.

The implementation issues locates on when the number of users and items are in large scale. In particular, given $m = |U|, n = |I|$, we have correlation between two users is $O(n)$; all correlations for a user is $O(mn)$; all pairwise correlation is $O(m^2n)$; recommendations take at least $O(mn)$. 

There are lots of ways to make computation practical: more persistent neighborhoods ($m \to k$); cached or incremental correlations.

\subsection{Configuring User-User Collaborative Filtering}
\paragraph{Selecting Neighborhoods:}
We can use all the neighbors; we can threshold similarity or distance, say 0.1 fraction of neighbors; we can random pick some number of neighbors; we can pick top-N neighbors by similarity or distance; we can also cluster and users and define neighbors. 

In theory, we want more neighbors. But in practice, noise can be made from dissimilar neighbors decreases usefulness. \# neighbors between 25 and 100 is often used.

\paragraph{Scoring Items from Neighborhoods:} We can use average scores or weighted average. 

\paragraph{Normalizing Data:} The reason doing normalization is because users rate differently (in scale). We can substract user mean rating or convert to z-score or substract item or item-user mean as we did in the main contents. 

\paragraph{Computing Similarities:} 1. Pearson correlation (Little data will make the similarity go to 1); 2. Spearman rank correlation; 3. Cosine similarity. 

\paragraph{Good Baseline Configuration:} Top N neighbors ($\sim$30); weighted averaging; user-mean or z-score normalization cosine similarity over normalized ratings.

\section{Item-Item Collaborative Filtering}
\paragraph{Motivations}
UUCF was great. But when the item sets is very large, the number of products that given customer would have rated was often way too small. Then a user simply didn't have anything in common with anybody else, which makes UUCF recommends nothing. Here comes a IICF when the number of items is relatively large or could be larger than the number of customers. Besides, recall the pairwise correlation computations in UUCF is $O(m^2 n)$, where $m$ is the number of users. This will be very expensive when $m$ is large. Even incremental approaches were expensive. And user profiles could change quickly so we need to compute in real time to keep users happy. 

\textbf{References:} Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2001. Item‐based collaborative filtering recommendation algorithms. In Proceedings of the 10th international conference on World Wide Web (WWW '01). ACM, New York, NY, USA, 285‐295.

\paragraph{Insight} Item-Item similarity is fairly stable, meaning for example, people like white legal pads is very likely to like yellow legal pads as well. Average item has many more ratings than an average user. Intuitively, items don't generally change rapidly -- at least not in ratings space (special case for time-bound items).

\paragraph{Benefits of Item-Item} 1. It actually works quite well in predicting accuracy and performs well on top-N predictions; 2. It has efficient implementation at least in cases where $|U| >> |I|$ and it has benefits of precomputability; 3. It has broad applicability and flexibility -- as easy to apply to a shopping cart as to a user profile. 

\paragraph{Core assumptions/limitations} Item-item relationships need to be stable (could have special cases that are difficult, e.g., calendars, short-lived books, Christmas trees, etc.). The main limitation/complaint is lower serendipity which may appear in UUCF. 

\subsection{Main Contents}
Two step process: 1. compute similarity between pairs of items, such as correlation or cosine of item rating vectors; 2. predict user-item rating using for instances, weighted sum of item neighbors or linear regression.

Notations are used in the same way in UUCF.
\begin{align*}
    s(u,i) = \frac{\sum_{j \in N} w_{i,j} r_{u,i}}{\sum_{j \in N} |w_{i,j}|},
\end{align*}
where $w_{i,j}$ now is the similarity between two items $i$ and $j$, which can be computed in the following ways.
\begin{align*}
    w_{i,j} = sim(i,j) = cos(\vec{\hat{r}}_i, \vec{\hat{r}}_j) = \frac{\vec{\hat{r}}_i \cdot \vec{\hat{r}}_j}{\|\vec{\hat{r}}_i\|_2 \|\vec{\hat{r}}_j\|_2} = \frac{\sum_u \hat{r}_{u,i} \hat{r}_{u,j}}{\sqrt{\sum_u \hat{r}_{u,i}^2}\sqrt{\sum_u \hat{r}_{u,j}^2}} = \frac{\sum_u (r_{u,i}-\bar{r}_i) (r_{u,j}-\bar{r}_j)}{\sqrt{\sum_u (r_{u,i}-\bar{r}_i)^2}\sqrt{\sum_u (r_{u,j}-\bar{r}_j)^2}}.
\end{align*}
Now the next question is how to to pick the neighbors $N(i;u)$. The most simple way is probably that we can look at the $k$ most similar items to $i$ that $u$ has rated. A good value of $k$ is important. A small k causes inaccurate scores; a large $k$ introduces noise. Usually, we can choose $k=20$. Combining these together, we get 
\begin{align*}
    s(u,i) = \frac{\sum_{j \in N(i;u)} w_{i,j} (r_{u,j} - \bar{r}_j)}{\sum_{j \in N(i;u)}|w_{i,j}|} + \bar{r}_i.
\end{align*}

\paragraph{Computation} We can pre-compute similarities for all pairs of items. Naively, the cost is $O(|I|^2)$. In face, we don't need to keep the whole $I^2$ model. We may just store the positive similarity and then clear all but $M >> k$ number of largest values.

\paragraph{IICF extension} We can incorporate user trustworthiness into item relatedness computation: $w_{i,j} = \frac{\sum_u p_u \hat{r}_{u,i} \hat{r}_{u,j}}{ \sqrt{\sum_u p_u \hat{r}_{u,i}^2}\sqrt{\sum_u p_u \hat{r}_{u,j}^2}}$. We can also incorporate the importance of the items, say papers and Page Rank. The similarity can be also learnt directly ML method. 

\paragraph{Strengths and weaknesses of IICF} Item-Item can be more efficient than User-User; Item-Item is an aggregated product-association recommender. Amazon used Item-Item widely. But MovieLens complained that the recommendation is too obvious. Item-Item is very difficult to discover highly different items to recommend; while User-User by default will elevate items that a close neighbor lovers, even without much evidence. Another consequence was that Item-Item predictions tended to be less extreme (since they were grounded in more data).
\end{document}

